---
title: "Optimization in ML"
layout: post
tags: machine learning
---

Some theoretical properties of GANS
------------
* Speaker: Gérard Biau

* The goal of GANs is, given a set of training images, generate a set of unseen images.
* Original paper, [Generative Adversarial Nets](https://papers.nips.cc/paper/5423-generative-adversarial-nets) (Goodfellow et al). 
* Most of GAN research is in optimization, and few about statistical properties.

* The generators are neural networks. The discriminators are functions from E to [0,1].
* The target function is an adversial function that contains parameters of the generator (that try to minimize the likelihood) and the discriminator (trat try to maximize it). 
* Multiple costs are possible, which gives raise to a galaxy of possible GANs.

### To read

* [Jensen Shanson Divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence) 
* [Generative Adversarial Nets](https://papers.nips.cc/paper/5423-generative-adversarial-nets)


--------

* When the data is separable, LogLoss has lots of local minima. In this setting SGD solves a SVM problem! (see Gunasekar, Lee, Soudry and Srebro 2018)

### To read 

* [Characterizing Implicit Bias in Terms of Optimization Geometry
](http://proceedings.mlr.press/v80/gunasekar18a.html)


## Mathematics of Deep Learning 
* Speaker: Réné Vidal

DEEP LEARNING

* Around 2000, SVM was beating NN since it gave similar errors, but has a global optimum and therefore it was easier to characterize its generalization properties, etc
* In Imagenet, non-Deep Learninig methods are getting 20% error. DL has decreased from 15% to 5% (2012-2016). Similar successes in Speech Recognition, Game Playing...
* Why the improvements ni DL:
   * Featres learned rather than hand-crafted.
   * More layers capture more invariances (e.g.: face rotations).
   * More data to train deeper networks.
   * More computing (GPUs).
   * Better regularization (Dropout).
   * New nonlinearities (Max Pooling, Rectified Linear Units). A partial explanation for ReLU is that gradient is larger and it reduces the vanishing gradient problem (but that's a very insatisfactory explanation).
* Are the principles ways to build networks?  How many layers, what activation functions? How many neurons in each layer?
* NN with single hidden layers are universal approximators (Cybenko 89, Hornik 89, Hornik 91, Barron 93) Why would I need DNN then? The mistery of DNN: Classic theory say that we need at least $O(M^2)$ examples to learn M parameters. In DNN practice we see good generalization with for instance O(M/60). Why? Recent works suggesting that maybe the number of parameters is not a good measure of the complexity.

* Droput is equivalent to regularization with products of weights.
* If the size of the network is large enough, a local minimum is a local minimum.
* There are architectures that facititate optimization.


MATRIX FACTORIZATION

* Typically the loss is convex, and the regularizator is convex.
Convex formularions $\mathcal{l}(Y,X) + \lambda |||X|_*$
* Factorized formulations are non-convex \mathcal{l}(Y, UV) + reg.
* Relation between the two: 
    * Structured low rank matrix factorization. Optimality, and Applications to Image Processing.
    * 
    * The idea is that the convex related problem has a global optimum. Then the only task is to increase the number of columns to increase the capacity until we capture the global optimum. After that, more capacity is not needed and extra columns are regularized. 

GLOBAL OPTIMAITY IN POSITIVE HOMOGENEUS FACTOROZATION

* How can we generalize the former ideas to DL?
* The DL problem is non-convex.
* A two later NN as a MF: $\psi(VX_1) X_2^T$

## Computing NMF

* NMF is a linear dimensinonallity reduction for nonnegative data

$$
argmin_{U,V}~~||M - UV||^2_F
$$

* Non-linear optimization problem with potentially many minima.
* NMF is NP-hard and ill-posed (non_uniqueness).
* Closely related to the nested polytopes problem.
* Most NMF algos come with no guarantees (except convergence)
* Solution in general is highly non-unique.
* Algo: Block coordinated Descent. Can be accelerated in multiple ways.
* If M is binary and we use l_1 norm (Robust NMF), the optimal solution is U and V binaries. 

#### To read

* On the computing of Robust PCA and l_1 norm Low-Rank Matrix Approximation. Mathematics of Operation Research 2018.


## Bayesian Optimization and Gaussian process bandits: Theory and Applications
* Speaker: A. Krause.

MOTIVATION

* Numerous applications require trading experimentation (exploration) and optimization (exploitation).
* Often there are much more alternative than trials, and experiments are noisy and expensive.
* But similar alternative have similar performance: can we exploit this?

K-ARMED BANDITS

* Sequentially allocate T tokens to k arms of a slot machine.
* Each time, pick arm and get payoff with unknown mean f_i.
* Want to maximize the expected cumulative reward.

* In many cases we have infinite arms. Classical algos don't work!
   * A lot of literature on Structured Bandits.
   * Bayesian Optimization as al alternative to Bandits.

* Bandits have strong theory, but they are less flexible.

* Compute cumulative regret so far.
   

## Statistical Optimality of Stochastic Gradient Descent with Multiple Passes
* Speaker: Francis Bach.

* In large datasets, process obs. one by one.
* Single pass SGD is optimal (theory)
* Multiple pass SGD always works better (practice)

* Usually ni large data, the covariance matrix has eigenvalues that decrease linearly in log log. The sum of the eigenvalues is smaller than its number.

### To read (others)
* [http://proceedings.mlr.press/v80/safran18a.html](Spurious Local Minima are Common in Two-Layer ReLU Neural Networks)
