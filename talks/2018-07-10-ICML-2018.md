---
layout: post
title: ICML 2018 notes
tags: machine learning
---


Variational Bayes and Beyond (Tamara Broderick)
-----

In these days, we have tons on data and two main probabilistic inference methods: Monte Carlo (especially Markov Chain Monte Carlo (MCMC))and Variational Inference (VI). MCMC are assymptotically exact but slow. VI is fast but biased by design. In VI, we find, among the set of "nice" distributions, the one that is closest to the true posterior in terms of KL divergence.

But, why KL divergence?

If I had an optimizer that gave me the approximated posterior q that minimizes KL, why this is not enough? Because I don't know the exact posterior. With a little manipulation, we obtain that KL is the evidence plus the ELBO. And that the ELBO does not contain the true posterior, but the joint likelihood, which we know. Since the KL is nonnegative, we can show that the log of the evidence upper bounds the ELBO. 

Why KL in that direction KL(q(.) || p(.))? 

Because we cannot pull this trick in the other direction (the algorithm for the other direction is Expectation Propagation).

And what are nice distribution?

Mean field. (MFVB)

Until, here, classic KL.

But the MFVB is NOT a modelling assumption. Variational Approximation with KL divergence with mean-field approximation with Coordinate Ascent optimization... this are a lot of choices! This restricts us to models that are super boring.

The variance of MFVB can be terribly understimated in many cases. And the more correlated the variables are, the worst. And a similar story for the mean.

A related paper: Yes, but Did it worl? Evaluating Variational Inference ICM Wedn 5pm

* Automated Scalable Basian Inference through data summarization.

Redundandy exists in datasets, even if data is not tall (tall = a lot of examples with respect to the dimensions). Can we just make a summary of this dataset and then apply a Monte Carlo method?

The idea is to find a "core (weighted) dataset (for Bayesian inference)". There were this before like "data squashing", but they did not come with theoretical guarantees. Another option is subsampling, but then elements from a minority sampled could dissapear from my dataset. And we want something that works in general.

We want to put a weight to each point so that $$ ||w||_0 < N $$ the sum of the weights is less than the total number of datapoints N (a lot of zero weights).

* Again, what about uniform subsampling? 

The posterior mean of each experiment will be completely different, but we know that the mean of the whole dataset is only one... Yeah, we can do it many times and then average these means. But this is a lot of samples and we gained nothing.

* Importance sampling.
Let us put some more weight in the minority class. What is the optimal weight? 

Then we sample from these importance weight and then invert the weights. Is this a good idea?

There is some theory tall tells us how good is that.

M is the number of points.

$$
||\mathcal{L}(w) - \mathcal{L} || \leq \frac{}{\sqrt{M}}
$$

But $$ 1/\sqrt{M} $$ is just the Monte Carlo error in importance sampling, so I gained nothing.


And how we optimize it? We use Frank-Wolfe optimization. But Frank-Wolfe optimizes over polytopes, and our problem setting is not because the of the 0 norms. But we can relax it.
And then

$$
||\mathcal{L}(w) - \mathcal{L} || \leq \frac{}{\sqrt{M}}
$$


Note that if model is from Exponential Family, we can use sufficient statistics to re-compute this part of the likelihood. BUt this is not always the case.


Recommended papers:


T Campbell and T Broderick. Automated scalable Bayesian inference via Hilbert coresets. Under review. ArXiv:1710.05053

JH Huggins, T Campbell, and T Broderick. Coresets for scalable Bayesian logistic regression. NIPS 2016.!

T Campbell and T Broderick. Bayesian Coreset Construction via Greedy Iterative Geodesic Ascent. ICML 2018, to appear.!
http://proceedings.mlr.press/v80/campbell18a/campbell18a.pdf


Agrawal ICML 2018 for reduction in combinatorial cases


http://www.tamarabroderick.com/tutorial_2018_csml.html


A stable learning algorithm
---------

Given a set of data points and a learning algorithm, removing a point does not changes the result so much.
An algorithm is $$\epsilon$$ - stable if the expectation of the difference of f(complete data) - f(sampled data) is smaller than epsilon (theorem Busquet et al 2002).
In SGD, we uniformy sample at each iteration. Hardt et al gave results for epsilons in convex and non-convex cases. But the constants in these epsilons are too high. They are worst case constants. But can we tighten this considering some locality or the distribution of the data?
The thing is 

Stability and Generalization of Algorithms that converge to global optima
-----------
Test error can be decomponsed in Trainig error plus geenralized error. I want to minimize both.
* VC theiry says that with high probability, the 

Paper: understanding deep learning requieres rethinking generalization.

SGD with losses is stable, also ERM....

Issues/:
* algo specific theory.
* we want to deal with non convex functions.


Plugging q convergecne rate and a functon class to a black box pof bounda gives a stability bounds (see their paper)


Variational Inference and Model Selection with Generalized Evidence Bounds
-----------

The goal is to generalized the classic ELBO to use it in model selection. He says MLE is equivalent to min KL (wtf? always?)

He sees the ELBO as a prior regularizator. The idea is to give more weight to low evidence samples so that we encourage fitting in them.

Also, the better logconcavity, the better the ELBO we can construct. So they generalize the ELBO with the importance weights.


Fixing a broken ELBO
--------------------
ELBO = encoder decoder prior. 



Stein Variational Gradient Descent Without Gradient
-----------
Uses the Stein operator.

Mini batch Gibbs Sampling on Large Graphical Models (poster 145)
--------------
There is minibatch for SGD.
Can we do the same for Gibbs Sampling? (with theoretical guarantees).

A factor graph is a distribution writen as a product of factors.
1/Z exp(\sum_i \phi_i(X1,..,x_n)) where x are variables. Each factor only depends on a subset of variables.
If I do minibacth the chain is not reversible.

On nesting MC estimators (poster 129)
-----------------------
Expectation of a function of an Expectation. Nested MC are not yet well understood.


Pathwise derivatives beyond the reparameterization  (poster 149)
------------
* Alternative to reparametration trick inspired by optimal transport (because the reparam trick, or what comes after, is not always applicable, for instance in Gamma or Beta distributions)


Semi-implicit Variational Inference (poster 177)
------------
Modern VI can use reparametrization instead of Mean Field. For the computation, we can use a NN. Implicit distribution consists of randomnes q(e).
But often we cannot integrate it. Solution proposed is SIVI


Augment and Reduce (Francisco Ruiz, Titsias...) (poster ??)
Categorical distributions
-----------

Usually softmax transformations from reals into probabilities. This is only one way, there is multinomial probit and many others.
Large number of outcomes (K>>1) such as language models, recommender systems...
The isssue is that large categorical demand large comput. costs.
In softmax e.g. the cost of computing the gradient is linear with K, and we have to do it for each iteration!
We can use minibatches etc.

We will augment the model with aux variable (and error term) and then subsampling.


**********Tigher Variational Bounds are not necessarely better (poster 193)
---------------------
Bu increasing samples can degrade learning.
IWAE. More samples reduce that variance but also the expected deviation.


**********Quasi Monte Carlo Variational  (Florian Wenzel, Disney Research) (poster 54)
-----
After the reparam trick, p(e) is my proposal distribution. Imagine this is the Uniform. We have a large variance. We can use QMC, deterministic, but the esimator is biased.
The idea is to reintriduce randomnes, RQMC. It reduces variance at least one order of magnitude. Also, they gradually increase the sample size (cool!!!!!!!!)

***********Yes, but did it work? (Gelman) (poster 150)
-------------
To what extent can I trust a VI approximation.

We need diagnosis:
 - has the objective function converged to a local optimum?
    Heuristic check (it becomes stable)
    Hypotheis testing.

The introduce two new diagnosis tools. PSIS, one of them, implies a Renyi Divergence with order 1/k.


******Variation reduction Langevin Monte Carlo
---
Stochastic Langevin MC simple, asymptoticaly extact. But needs the full gradient at each step.
Welling Teh 2011 proposed replace it by an estimate of the gradient.


********Bayesian coreset construction via Greedy... (poster 160)
----------------
* Goal: Posterior summarization (mean, variance...)
* Total likelihood is now a weighted sum of individual loglikelihoods.
* if all w=1, traditional.
* Before, he thought of likelihoods as vectors. The goal was to find a sparse approximation of a vector sum, the total L. Then he tried to minimize the posterior Fisher information norm suject to a cardinality constraint. And used Frank Wolfe approximation.
* Still, there is romm for improvement. Because for small coresets, Frank Wolf is as bad as subsampling. The worst case is normal means models (?)
* Solution, I can multiply L(w) by an alpha>0 and solve for alpha analytically. Scales L(w) to the norm of the vector we want to estimate.
* What is left? Direction. Uses GIGA (Greedy Iterative Geodesic Ascent) The code is not too hard to do. 
* GIGA bounds the error with a norm v^M where M is the size of the coreset.

github.com/trevorcampbell/bayesian-coresets


Mirar SVGD (Liu & Wand 2016)



**********Welling
-------
Jaynes: Entropy is our ignorance.
Rissanen (MDL) The total info is the bu√±ber of bits to encode your model hypothesis (KL) and then the bits to encode the data given the hypothesis (E[log joint]).
Hinton: Variational formulation of the free energy (free energy (of the model) = - energy + entropy)

Can we use the concept of entropy to more efficient ML? Use less power in our machines.

MCMC failts the Big Data test: at each iteration, you have to go through all your data.
Solution: Stochastic Gradient Langevin Dynamics (Weling, Teh. Bayesian Learning via stochastic gradient Langevin dynamics)
Langeving is Gradient Ascent where we add a Normal noise in the parameter update
Stochastic version does not need Metropolis accept step.

Note: reparatremization trick in VI is an alternative to mean fields. This trick reduces the variance.

* "The economic benefit of AI must be greater than the energy cost."

"AI algos will be measured by the amount of energy they provide per Kilowathour"


Probabilistic Boolean Tensor Factorization
-----------
* I have a genes x genes matrix for each individual (gene relative expression in cancer)
* Bernouilli. Probability of 1 is q if Z and U share some topic. 1-q otherwise. (a logistic or)
(en mi caso, si tienen un topic en comun la prob es 1 en el caso Dir Dir, o correlaci√≥n z*u en general para DirBeta)
* Posterior inference


Implicit regularization
--------------------------

Are non regularized methods intrinsecly suboptimal for non convex problems?


------------
ICA for binary components Anankumar 2014
Smawski 2014 @NIPS


* Louis
-------
KL generalized a cost function... se ha liao.

D_KL <-> p() habria que haber puesto Poisson()
Ha puesto las negritas que le ha salido del conyo

Teniamos que haber hecho un resumen al principio para decir rapido lo de la self regularization.
La indetermancy the the betano ha quedado clara.

H se puede marginalizar gracias a la CONJUGACY!!!

Un poco lioso todo, espaghetti.

Teniamos que hacer esenyado una imagen de sparsity al principio para que no se pierda el objetivo.

Unfortunately we dont have explanaition for C better?? Joder algo si que tenemos!!!!!!!!!!! No ha nombrado overdispersion.

Pregunta: como elegimos un bien prior? Pq frequentist approaches DONT need any assumption, FALSOOOOOOOOO
La tipa tiene un paper sobre el tema.



VARIATIONAL AUTOENCODERS
__-----------------------------


ELBO la descompone eb negative resconstruction error y regul penalty
	
Eq[log p (x | z)] - KL (q | p(z))  ---- que es p(z)? El prior!!!
Si q es gaussiana, en el medio de la red tendre un q(z | x) Gaussiano y la KL sera la distancia con el prior.
Osea estoy diciendo de aproximar mis datos x con una gaussiana.

tengo un gradente de una esperanza, y no se como hacerlo. Con truco score function, regla de cadena en el fondo, puedo pasarlo
a la esperanza de un gradiente de la function, y ese gradiente de la function si se resolverlo en close form.

Blei keynote en AISTATS 2018

Kingma and Dharwal 2018

VAE are likelihood-based
GAN are likelihood-free