---
layout: ai-note
title: Direct Preference Optimization (DPO)
category: "Agents"
date: 2024-10-31
tags: reinforcement-learning, llm, preference-learning
---

ðŸš§ **Work in progress...**

This article will cover Direct Preference Optimization (DPO), a simpler alternative to RLHF that directly optimizes language models using preference data without requiring a separate reward model.

## Topics to cover:
- Limitations of RLHF and PPO
- DPO formulation and theoretical foundation
- Comparison with PPO-based RLHF
- Implementation and practical considerations
